{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.utils import class_weight as sk_class_wgt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer as Normalizer\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer,PowerTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from datetime import datetime \n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14376, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df01=pd.read_csv(r'C:\\Users\\tekin\\Desktop\\ML\\Tweedy\\TechLemma_D1906.csv',index_col=False)\n",
    "#print(dt_cl[:300])\n",
    "df01.shape\n",
    "#posset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(min(df01['text'].values,key=len))\n",
    "#len(df01[df01['text'].map(len)> 340])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edInput</th>\n",
       "      <th>editor</th>\n",
       "      <th>usScName</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>Get peeling @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>barbwire deploy @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>clean laser @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>personal drone @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>blinker bike @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>firework feel @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>future transportation @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>clock eyeful @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>golden hour @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>average car @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>clean Keurig @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>massager like @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>terrify all @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>building unlike @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>Willy Wonka @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3541</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>Ronald_vanLoon</td>\n",
       "      <td>setup blockchain @ronald_vanloon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>hovercraft almost @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3819</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>cheesy world @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>verge</td>\n",
       "      <td>Microsoft Cortana @verge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>techreview</td>\n",
       "      <td>likely begin @techreview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4052</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>boat drone @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>Incognito Mode @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4153</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>play @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4406</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>Yea confuse @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4501</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>kashthefuturist</td>\n",
       "      <td>think Firevase @kashthefuturist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4587</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>reset #iphone @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>bike future @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4834</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>nee robot @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>tell difference @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>Alexa Matrix @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7233</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>futureshift</td>\n",
       "      <td>Well twist @futureshift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7425</th>\n",
       "      <td>1</td>\n",
       "      <td>5007</td>\n",
       "      <td>mashable</td>\n",
       "      <td>really surprise @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7757</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>strong @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7835</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>hovercraft awesome @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7913</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>ultimate 4x4 @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8071</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>techreview</td>\n",
       "      <td>computer toilet @techreview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8084</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>fold right @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8254</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>hockey treadmill @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8264</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>build vacuum @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8441</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>recycle mattress @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8465</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>kashthefuturist</td>\n",
       "      <td>travel newborn @kashthefuturist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8663</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>trash take @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8727</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>continue terrify @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8905</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>drone surf @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9836</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>window balcony @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>photo joke @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10287</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>fitness motivation @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10516</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>kashthefuturist</td>\n",
       "      <td>cyborg future @kashthefuturist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10653</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>grow @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10728</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>get easy @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10845</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>robot shovel @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10856</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>college @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11531</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>electric supercar @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11543</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>finger thank @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11635</th>\n",
       "      <td>1</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>sculpture seem @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12962</th>\n",
       "      <td>0</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>excite @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13151</th>\n",
       "      <td>0</td>\n",
       "      <td>5008</td>\n",
       "      <td>DigitalTrends</td>\n",
       "      <td>glaci kayak @digitaltrends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13168</th>\n",
       "      <td>0</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>italian translator @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13248</th>\n",
       "      <td>0</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>@mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13501</th>\n",
       "      <td>0</td>\n",
       "      <td>5008</td>\n",
       "      <td>mashable</td>\n",
       "      <td>one @mashable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       edInput  editor         usScName                               text\n",
       "703          1    5008    DigitalTrends         Get peeling @digitaltrends\n",
       "801          1    5008    DigitalTrends     barbwire deploy @digitaltrends\n",
       "952          1    5007    DigitalTrends         clean laser @digitaltrends\n",
       "1039         1    5008    DigitalTrends      personal drone @digitaltrends\n",
       "1174         1    5008    DigitalTrends        blinker bike @digitaltrends\n",
       "2311         1    5007         mashable            firework feel @mashable\n",
       "2605         1    5008         mashable    future transportation @mashable\n",
       "2644         1    5007         mashable             clock eyeful @mashable\n",
       "2876         1    5007    DigitalTrends         golden hour @digitaltrends\n",
       "2898         1    5007         mashable              average car @mashable\n",
       "3041         1    5007    DigitalTrends        clean Keurig @digitaltrends\n",
       "3309         1    5007         mashable            massager like @mashable\n",
       "3314         1    5007         mashable              terrify all @mashable\n",
       "3344         1    5007         mashable          building unlike @mashable\n",
       "3486         1    5007         mashable              Willy Wonka @mashable\n",
       "3541         1    5007   Ronald_vanLoon   setup blockchain @ronald_vanloon\n",
       "3641         1    5008         mashable        hovercraft almost @mashable\n",
       "3819         1    5007         mashable             cheesy world @mashable\n",
       "3825         1    5007            verge           Microsoft Cortana @verge\n",
       "4006         1    5007       techreview           likely begin @techreview\n",
       "4052         1    5007    DigitalTrends          boat drone @digitaltrends\n",
       "4083         1    5007    DigitalTrends      Incognito Mode @digitaltrends\n",
       "4153         1    5007         mashable                     play @mashable\n",
       "4406         1    5007         mashable              Yea confuse @mashable\n",
       "4501         1    5007  kashthefuturist    think Firevase @kashthefuturist\n",
       "4587         1    5007    DigitalTrends       reset #iphone @digitaltrends\n",
       "4646         1    5007         mashable              bike future @mashable\n",
       "4834         1    5007         mashable                nee robot @mashable\n",
       "4844         1    5007         mashable          tell difference @mashable\n",
       "4957         1    5007         mashable             Alexa Matrix @mashable\n",
       "...        ...     ...              ...                                ...\n",
       "7233         1    5007      futureshift            Well twist @futureshift\n",
       "7425         1    5007         mashable          really surprise @mashable\n",
       "7757         1    5008         mashable                   strong @mashable\n",
       "7835         1    5008    DigitalTrends  hovercraft awesome @digitaltrends\n",
       "7913         1    5008    DigitalTrends        ultimate 4x4 @digitaltrends\n",
       "8071         1    5008       techreview        computer toilet @techreview\n",
       "8084         1    5008         mashable               fold right @mashable\n",
       "8254         1    5008    DigitalTrends    hockey treadmill @digitaltrends\n",
       "8264         1    5008    DigitalTrends        build vacuum @digitaltrends\n",
       "8441         1    5008    DigitalTrends    recycle mattress @digitaltrends\n",
       "8465         1    5008  kashthefuturist    travel newborn @kashthefuturist\n",
       "8663         1    5008    DigitalTrends          trash take @digitaltrends\n",
       "8727         1    5008         mashable         continue terrify @mashable\n",
       "8905         1    5008    DigitalTrends          drone surf @digitaltrends\n",
       "9836         1    5008    DigitalTrends      window balcony @digitaltrends\n",
       "10006        1    5008         mashable               photo joke @mashable\n",
       "10287        1    5008         mashable       fitness motivation @mashable\n",
       "10516        1    5008  kashthefuturist     cyborg future @kashthefuturist\n",
       "10653        1    5008         mashable                     grow @mashable\n",
       "10728        1    5008         mashable                 get easy @mashable\n",
       "10845        1    5008    DigitalTrends        robot shovel @digitaltrends\n",
       "10856        1    5008         mashable                  college @mashable\n",
       "11531        1    5008    DigitalTrends   electric supercar @digitaltrends\n",
       "11543        1    5008         mashable             finger thank @mashable\n",
       "11635        1    5008         mashable           sculpture seem @mashable\n",
       "12962        0    5008         mashable                   excite @mashable\n",
       "13151        0    5008    DigitalTrends         glaci kayak @digitaltrends\n",
       "13168        0    5008         mashable       italian translator @mashable\n",
       "13248        0    5008         mashable                          @mashable\n",
       "13501        0    5008         mashable                      one @mashable\n",
       "\n",
       "[73 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drpix=[i for i in df01.index if len(df01.text[i].split())< 4]\n",
    "#df01[~df01.index.isin(drpix)]\n",
    "df01[df01.index.isin(drpix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df01['text'].values,key=len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vect=np.array(df01.edInput)\n",
    "text_vect=np.array(df01.text[:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train_all, y_test =train_test_split(text_vect,\n",
    "            y_vect,random_state=0,test_size=0.1,stratify=y_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use count vectorizer then tfid\n",
    "#!!!!!!!!! we can add a column with a flag value that shows the tweet has pic or video into word vector\n",
    "\n",
    "#lemma_vect =  TfidfVectorizer(tokenizer=None,token_pattern='\\S+',\n",
    "#                             min_df=4,lowercase=False,stop_words=None,ngram_range=(1,4),max_features=12000)\n",
    "lemma_vect =  CountVectorizer(tokenizer=None,token_pattern='\\S+',\n",
    "                             min_df=4,lowercase=False,stop_words=None,ngram_range=(1,6),max_features=12000)\n",
    "X_train_tmp=lemma_vect.fit_transform(X_train_raw)\n",
    "X_test=normalizer.transform(X_test_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12938, 12000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tmp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2019/04/normalization-vs-standardization-quantitative-analysis.html\n",
    "\n",
    "Best classifier from each model:\n",
    "\n",
    "    SVM + StandardScaler : 0.849\n",
    "    MLP + PowerTransformer-Yeo-Johnson : 0.839\n",
    "    KNN + MinMaxScaler : 0.813\n",
    "    LR + QuantileTransformer-Uniform : 0.808\n",
    "    NB + PowerTransformer-Yeo-Johnson : 0.752\n",
    "    LDA + PowerTransformer-Yeo-Johnson : 0.747\n",
    "    CART + QuantileTransformer-Uniform : 0.74\n",
    "    RF + Normalizer : 0.723\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix \n",
    "sclPT=PowerTransformer()\n",
    "#sclSTD=StandardScaler()\n",
    "#normalizer=Normalizer()\n",
    "#without normalizing\n",
    "X_train_scl=sclPT.fit_transform(X_train_tmp.toarray())\n",
    "#X_train_scl=X_train_tmp\n",
    "\n",
    "X_test=sclPT.transform(X_test_tmp)\n",
    "#X_test=X_test_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svd= TruncatedSVD(n_components=Dim_num)\n",
    "\n",
    "\n",
    "#X_train_scl = svd.fit_transform(X_train_scl)\n",
    "#X_test = svd.transform(X_test_scl)\n",
    "#print(svd.explained_variance_ratio_.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section is to print the names of the best features obtained from SVD\n",
    "#check the names see if they are reasonable with your category\n",
    "\n",
    "#feature_names=lemma_vect.get_feature_names()\n",
    "#best_features = [feature_names[i] for i in svd.components_[0].argsort()[::-1]]\n",
    "#print(best_features[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_scl, y_train_all, \n",
    "                                                  test_size=0.1,stratify=y_train_all,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_raw\n",
    "del X_train_tmp\n",
    "#del X_train_scl\n",
    "#del X_train_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.SpecificityAtSensitivity(0.85,name='spcsens'),\n",
    "      keras.metrics.SensitivityAtSpecificity(0.70,name='snsspec')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584 \n",
      " 12792\n",
      "Weight for class 0: 4.54\n",
      "Weight for class 1: 0.56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.11674017])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WEIGHTED MODEL\n",
    "\n",
    "negative = len(df01[df01['edInput'] == 0])\n",
    "positive = len(df01[df01['edInput'] == 1])\n",
    "print(negative,'\\n',positive)\n",
    "total = negative + positive\n",
    "weight_for_0 = (1 / negative)*(total)/2.0 \n",
    "weight_for_1 = (1 / positive)*(total)/2.0\n",
    "#weight_for_1 = 100\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "val_sample_weights = sk_class_wgt.compute_sample_weight(class_weight, y_val)\n",
    "train_sample_weights = sk_class_wgt.compute_sample_weight(class_weight, y_train)\n",
    "test_sample_weights = sk_class_wgt.compute_sample_weight(class_weight, y_test)\n",
    "initial_bias = np.log([positive/total])\n",
    "initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateDecay:\n",
    "    def plot(self, epochs, title=\"Learning Rate Schedule\"):\n",
    "        # compute the set of learning rates for each corresponding\n",
    "        # epoch\n",
    "        lrs = [self(i) for i in epochs]\n",
    "        # the learning rate schedule\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, lrs)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        \n",
    "class PolynomialDecay():\n",
    "    def __init__(self, maxEpochs=100, initAlpha=0.01, power=1.0):\n",
    "        # store the maximum number of epochs, base learning rate,\n",
    "        # and power of the polynomial\n",
    "        self.maxEpochs = maxEpochs\n",
    "        self.initAlpha = initAlpha\n",
    "        self.power = power\n",
    "    def __call__(self, epoch):\n",
    "        # compute the new learning rate based on polynomial decay\n",
    "        decay = (1 - (epoch / float(self.maxEpochs))) ** self.power\n",
    "\n",
    "        alpha = self.initAlpha * decay\n",
    "        print(\"decay:\",decay,\">> lr:\",alpha)\n",
    "        # return the new learning rate\n",
    "        return float(alpha)\n",
    "schedule = PolynomialDecay(maxEpochs=EPOCHS, initAlpha=3e-4, power=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#schedule.__call__(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bias = keras.initializers.Constant(initial_bias)\n",
    "def make_model(metrics = METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = keras.initializers.Constant(output_bias)\n",
    "    model = keras.Sequential([\n",
    "          keras.layers.Dense(\n",
    "              128, activation='relu',\n",
    "              input_shape=(X_train.shape[-1],),kernel_regularizer=regularizers.l2(0.01)\n",
    "              ,activity_regularizer=regularizers.l2(0.5)),\n",
    "          keras.layers.Dropout(0.05),\n",
    "          keras.layers.Dense(\n",
    "              64, activation='relu',\n",
    "              input_shape=(X_train.shape[-1],),kernel_regularizer=regularizers.l2(0.01)\n",
    "          ,activity_regularizer=regularizers.l2(0.05)),\n",
    "          keras.layers.Dropout(0.5),\n",
    "\n",
    "          keras.layers.Dense(1, activation='sigmoid',\n",
    "                         bias_initializer=output_bias),\n",
    "  ])\n",
    "\n",
    "#    model.compile(\n",
    "#      optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "#      loss=keras.losses.BinaryCrossentropy(),\n",
    "#      metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch=100\n",
    "ep=lambda epoch: 1e-8 * 10**(epoch / 20)\n",
    "float(ep(100))\n",
    "#10**(1/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr_schedule = keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "optimizer = keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
    "model.compile(loss=keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=METRICS)\n",
    "history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "plt.axis([1e-5, 1e-3, 0, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = make_model() \n",
    "model.compile(\n",
    "      optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=METRICS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpnt_filepath=r\"D:\\work\\SavedModels\\Tweedy\\Ckpt\\chkpntwghts.{epoch:02d}-{val_weighted_accuracy:.4f}.hdf5\" \n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_weighted_accuracy', \n",
    "    verbose=1,\n",
    "    patience=30,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=chkpnt_filepath,\n",
    "     save_best_only=True,  monitor=\"val_weighted_accuracy\")\n",
    "lrscheduler=keras.callbacks.LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = make_model()\n",
    "baseline_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (12, 12)\n",
    "plot_metrics(baseline_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "  cm = confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_predictions = model.predict(X_train, batch_size=BATCH_SIZE)\n",
    "test_predictions= model.predict(X_test, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results = model.evaluate(X_test, y_test,\n",
    "                                           batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(y_test, test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1536128   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,544,449\n",
      "Trainable params: 1,544,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "weighted_model = make_model(output_bias=initial_bias)\n",
    "#weighted_model = make_model()\n",
    "weighted_model.compile(\n",
    "      optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=METRICS,weighted_metrics=[METRICS[4]])\n",
    "weighted_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11644 samples, validate on 1294 samples\n",
      "decay: 1.0 >> lr: 0.0003\n",
      "Epoch 1/200\n",
      "11644/11644 [==============================] - 9s 776us/sample - loss: 2696.5729 - tp: 1312.0000 - fp: 120.0000 - tn: 1163.0000 - fn: 9049.0000 - weighted_accuracy: 0.8205 - precision: 0.9162 - recall: 0.1266 - auc: 0.5081 - spcsens: 0.1302 - snsspec: 0.3318 - val_loss: 3466.2348 - val_tp: 20.0000 - val_fp: 1.0000 - val_tn: 142.0000 - val_fn: 1131.0000 - val_weighted_accuracy: 0.5060 - val_precision: 0.9524 - val_recall: 0.0174 - val_auc: 0.4959 - val_spcsens: 0.1538 - val_snsspec: 0.3189\n",
      "decay: 0.995 >> lr: 0.0002985\n",
      "Epoch 2/200\n",
      "11644/11644 [==============================] - 8s 690us/sample - loss: 1416.6559 - tp: 202.0000 - fp: 12.0000 - tn: 1271.0000 - fn: 10159.0000 - weighted_accuracy: 0.8836 - precision: 0.9439 - recall: 0.0195 - auc: 0.5356 - spcsens: 0.1754 - snsspec: 0.3454 - val_loss: 2463.8873 - val_tp: 1.0000 - val_fp: 0.0000e+00 - val_tn: 143.0000 - val_fn: 1150.0000 - val_weighted_accuracy: 0.5013 - val_precision: 1.0000 - val_recall: 8.6881e-04 - val_auc: 0.4812 - val_spcsens: 0.0979 - val_snsspec: 0.2841\n",
      "decay: 0.99 >> lr: 0.00029699999999999996\n",
      "Epoch 3/200\n",
      "11644/11644 [==============================] - 8s 685us/sample - loss: 834.8129 - tp: 32.0000 - fp: 1.0000 - tn: 1282.0000 - fn: 10329.0000 - weighted_accuracy: 0.8895 - precision: 0.9697 - recall: 0.0031 - auc: 0.5568 - spcsens: 0.1980 - snsspec: 0.3646 - val_loss: 1822.6784 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 143.0000 - val_fn: 1151.0000 - val_weighted_accuracy: 0.5008 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4828 - val_spcsens: 0.1329 - val_snsspec: 0.2745\n",
      "decay: 0.985 >> lr: 0.00029549999999999997\n",
      "Epoch 4/200\n",
      "11644/11644 [==============================] - 8s 687us/sample - loss: 500.7079 - tp: 5.0000 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 10356.0000 - weighted_accuracy: 0.8899 - precision: 1.0000 - recall: 4.8258e-04 - auc: 0.5757 - spcsens: 0.2315 - snsspec: 0.4068 - val_loss: 1438.8669 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 143.0000 - val_fn: 1151.0000 - val_weighted_accuracy: 0.5008 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5079 - val_spcsens: 0.1538 - val_snsspec: 0.3015\n",
      "decay: 0.98 >> lr: 0.000294\n",
      "Epoch 5/200\n",
      "11644/11644 [==============================] - 8s 674us/sample - loss: 313.8740 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 10361.0000 - weighted_accuracy: 0.8898 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6039 - spcsens: 0.2354 - snsspec: 0.4467 - val_loss: 1196.3889 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 143.0000 - val_fn: 1151.0000 - val_weighted_accuracy: 0.5008 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5014 - val_spcsens: 0.1119 - val_snsspec: 0.2919\n",
      "decay: 0.975 >> lr: 0.00029249999999999995\n",
      "Epoch 6/200\n",
      "11644/11644 [==============================] - 8s 647us/sample - loss: 203.2654 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 10361.0000 - weighted_accuracy: 0.8898 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6466 - spcsens: 0.3141 - snsspec: 0.5036 - val_loss: 1023.6410 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 143.0000 - val_fn: 1151.0000 - val_weighted_accuracy: 0.5008 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5159 - val_spcsens: 0.1608 - val_snsspec: 0.3388\n",
      "decay: 0.97 >> lr: 0.00029099999999999997\n",
      "Epoch 7/200\n",
      "11644/11644 [==============================] - 8s 650us/sample - loss: 139.8684 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 10361.0000 - weighted_accuracy: 0.8898 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6701 - spcsens: 0.3157 - snsspec: 0.5397 - val_loss: 924.0602 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 143.0000 - val_fn: 1151.0000 - val_weighted_accuracy: 0.5008 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5579 - val_spcsens: 0.1818 - val_snsspec: 0.4292\n",
      "decay: 0.965 >> lr: 0.0002895\n",
      "Epoch 8/200\n",
      "11644/11644 [==============================] - 8s 650us/sample - loss: 99.9975 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 10361.0000 - weighted_accuracy: 0.8898 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7056 - spcsens: 0.3749 - snsspec: 0.5849 - val_loss: 847.7179 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 143.0000 - val_fn: 1151.0000 - val_weighted_accuracy: 0.5008 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5397 - val_spcsens: 0.2238 - val_snsspec: 0.3388\n",
      "decay: 0.96 >> lr: 0.00028799999999999995\n",
      "Epoch 9/200\n",
      "11644/11644 [==============================] - 8s 687us/sample - loss: 74.1683 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 10361.0000 - weighted_accuracy: 0.8898 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7437 - spcsens: 0.4209 - snsspec: 0.6193 - val_loss: 792.3928 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 143.0000 - val_fn: 1151.0000 - val_weighted_accuracy: 0.5008 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5609 - val_spcsens: 0.2098 - val_snsspec: 0.4144\n",
      "decay: 0.955 >> lr: 0.00028649999999999997\n",
      "Epoch 10/200\n",
      "11644/11644 [==============================] - 8s 695us/sample - loss: 56.6318 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 10361.0000 - weighted_accuracy: 0.8898 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7843 - spcsens: 0.4887 - snsspec: 0.7017 - val_loss: 736.0438 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 143.0000 - val_fn: 1151.0000 - val_weighted_accuracy: 0.5008 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6087 - val_spcsens: 0.2517 - val_snsspec: 0.4318\n",
      "decay: 0.95 >> lr: 0.000285\n",
      "Epoch 11/200\n",
      "11644/11644 [==============================] - 8s 693us/sample - loss: 43.7479 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 10361.0000 - weighted_accuracy: 0.8898 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7941 - spcsens: 0.5230 - snsspec: 0.6973 - val_loss: 693.9325 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 143.0000 - val_fn: 1151.0000 - val_weighted_accuracy: 0.5008 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5908 - val_spcsens: 0.2517 - val_snsspec: 0.4353\n",
      "decay: 0.945 >> lr: 0.00028349999999999995\n",
      "Epoch 12/200\n",
      "11644/11644 [==============================] - 8s 694us/sample - loss: 34.6368 - tp: 2.0000 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 10359.0000 - weighted_accuracy: 0.8898 - precision: 1.0000 - recall: 1.9303e-04 - auc: 0.8213 - spcsens: 0.5549 - snsspec: 0.7357 - val_loss: 653.8219 - val_tp: 11.0000 - val_fp: 2.0000 - val_tn: 141.0000 - val_fn: 1140.0000 - val_weighted_accuracy: 0.4986 - val_precision: 0.8462 - val_recall: 0.0096 - val_auc: 0.5710 - val_spcsens: 0.2308 - val_snsspec: 0.37880 - fp: 0.0000e+00 - tn: 435\n",
      "decay: 0.94 >> lr: 0.00028199999999999997\n",
      "Epoch 13/200\n",
      "11644/11644 [==============================] - 8s 691us/sample - loss: 27.6806 - tp: 29.0000 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 10332.0000 - weighted_accuracy: 0.8901 - precision: 1.0000 - recall: 0.0028 - auc: 0.8268 - spcsens: 0.5783 - snsspec: 0.7493 - val_loss: 614.1698 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 143.0000 - val_fn: 1151.0000 - val_weighted_accuracy: 0.5008 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5907 - val_spcsens: 0.2448 - val_snsspec: 0.4075 - weighted_accuracy: 0.8903 - precision: 1.0000 - recall: 0.0029 - auc: 0.8258 - spcsens: 0.574\n",
      "decay: 0.935 >> lr: 0.0002805\n",
      "Epoch 14/200\n",
      "11644/11644 [==============================] - 8s 701us/sample - loss: 22.5666 - tp: 46.0000 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 10315.0000 - weighted_accuracy: 0.8903 - precision: 1.0000 - recall: 0.0044 - auc: 0.8400 - spcsens: 0.6181 - snsspec: 0.7522 - val_loss: 574.6719 - val_tp: 62.0000 - val_fp: 4.0000 - val_tn: 139.0000 - val_fn: 1089.0000 - val_weighted_accuracy: 0.5137 - val_precision: 0.9394 - val_recall: 0.0539 - val_auc: 0.5748 - val_spcsens: 0.2238 - val_snsspec: 0.3684ghted_accuracy: 0.8858 - precision: 1.0000 - recall: \n",
      "decay: 0.9299999999999999 >> lr: 0.00027899999999999995\n",
      "Epoch 15/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11644/11644 [==============================] - 8s 702us/sample - loss: 18.6890 - tp: 212.0000 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 10149.0000 - weighted_accuracy: 0.8921 - precision: 1.0000 - recall: 0.0205 - auc: 0.8321 - spcsens: 0.6009 - snsspec: 0.7431 - val_loss: 538.9941 - val_tp: 365.0000 - val_fp: 44.0000 - val_tn: 99.0000 - val_fn: 786.0000 - val_weighted_accuracy: 0.5050 - val_precision: 0.8924 - val_recall: 0.3171 - val_auc: 0.5558 - val_spcsens: 0.1888 - val_snsspec: 0.3058\n",
      "decay: 0.925 >> lr: 0.00027749999999999997\n",
      "Epoch 16/200\n",
      "11644/11644 [==============================] - 8s 670us/sample - loss: 15.8134 - tp: 353.0000 - fp: 2.0000 - tn: 1281.0000 - fn: 10008.0000 - weighted_accuracy: 0.8922 - precision: 0.9944 - recall: 0.0341 - auc: 0.8348 - spcsens: 0.5869 - snsspec: 0.7383 - val_loss: 505.0711 - val_tp: 374.0000 - val_fp: 43.0000 - val_tn: 100.0000 - val_fn: 777.0000 - val_weighted_accuracy: 0.5124 - val_precision: 0.8969 - val_recall: 0.3249 - val_auc: 0.5502 - val_spcsens: 0.1538 - val_snsspec: 0.3258\n",
      "decay: 0.92 >> lr: 0.000276\n",
      "Epoch 17/200\n",
      "11644/11644 [==============================] - 8s 711us/sample - loss: 13.7184 - tp: 586.0000 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 9775.0000 - weighted_accuracy: 0.8960 - precision: 1.0000 - recall: 0.0566 - auc: 0.8374 - spcsens: 0.6306 - snsspec: 0.7715 - val_loss: 479.1404 - val_tp: 364.0000 - val_fp: 28.0000 - val_tn: 115.0000 - val_fn: 787.0000 - val_weighted_accuracy: 0.5606 - val_precision: 0.9286 - val_recall: 0.3162 - val_auc: 0.5835 - val_spcsens: 0.2098 - val_snsspec: 0.4188 - fn: 8217.0000 - weighted_accuracy: 0.8960 - precision: 1.0000 - recall: 0.0536 - auc: 0.8412 - spcsens: 0.6345 -  - ETA: 0s - loss: 13.7133 - tp: 558.0000 - fp: 0.0000e+00 - tn: 1238.0000 - fn: 9340.0000 - weighted_accuracy: 0.8970 - precision: 1.0000 - recall: 0.0564 - auc: 0.8362 - spcsens: 0.6284 - snsspec: \n",
      "decay: 0.915 >> lr: 0.0002745\n",
      "Epoch 18/200\n",
      "11644/11644 [==============================] - 9s 737us/sample - loss: 12.1540 - tp: 719.0000 - fp: 3.0000 - tn: 1280.0000 - fn: 9642.0000 - weighted_accuracy: 0.8954 - precision: 0.9958 - recall: 0.0694 - auc: 0.8395 - spcsens: 0.6196 - snsspec: 0.7566 - val_loss: 443.8779 - val_tp: 474.0000 - val_fp: 45.0000 - val_tn: 98.0000 - val_fn: 677.0000 - val_weighted_accuracy: 0.5488 - val_precision: 0.9133 - val_recall: 0.4118 - val_auc: 0.5712 - val_spcsens: 0.1818 - val_snsspec: 0.3814\n",
      "decay: 0.91 >> lr: 0.00027299999999999997\n",
      "Epoch 19/200\n",
      "11644/11644 [==============================] - 8s 689us/sample - loss: 10.9384 - tp: 729.0000 - fp: 2.0000 - tn: 1281.0000 - fn: 9632.0000 - weighted_accuracy: 0.8962 - precision: 0.9973 - recall: 0.0704 - auc: 0.8464 - spcsens: 0.6321 - snsspec: 0.7942 - val_loss: 413.4782 - val_tp: 470.0000 - val_fp: 50.0000 - val_tn: 93.0000 - val_fn: 681.0000 - val_weighted_accuracy: 0.5295 - val_precision: 0.9038 - val_recall: 0.4083 - val_auc: 0.5711 - val_spcsens: 0.1958 - val_snsspec: 0.3779\n",
      "decay: 0.905 >> lr: 0.0002715\n",
      "Epoch 20/200\n",
      "11644/11644 [==============================] - 8s 689us/sample - loss: 9.9491 - tp: 774.0000 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 9587.0000 - weighted_accuracy: 0.8980 - precision: 1.0000 - recall: 0.0747 - auc: 0.8612 - spcsens: 0.6571 - snsspec: 0.8032 - val_loss: 378.4590 - val_tp: 524.0000 - val_fp: 60.0000 - val_tn: 83.0000 - val_fn: 627.0000 - val_weighted_accuracy: 0.5179 - val_precision: 0.8973 - val_recall: 0.4553 - val_auc: 0.5541 - val_spcsens: 0.1608 - val_snsspec: 0.3223\n",
      "decay: 0.9 >> lr: 0.00027\n",
      "Epoch 21/200\n",
      "11644/11644 [==============================] - 8s 680us/sample - loss: 9.1714 - tp: 850.0000 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 9511.0000 - weighted_accuracy: 0.8989 - precision: 1.0000 - recall: 0.0820 - auc: 0.8640 - spcsens: 0.6812 - snsspec: 0.8182 - val_loss: 344.9818 - val_tp: 551.0000 - val_fp: 60.0000 - val_tn: 83.0000 - val_fn: 600.0000 - val_weighted_accuracy: 0.5297 - val_precision: 0.9018 - val_recall: 0.4787 - val_auc: 0.5573 - val_spcsens: 0.1538 - val_snsspec: 0.3267\n",
      "decay: 0.895 >> lr: 0.00026849999999999997\n",
      "Epoch 22/200\n",
      "11644/11644 [==============================] - 8s 692us/sample - loss: 8.4664 - tp: 1088.0000 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 9273.0000 - weighted_accuracy: 0.9014 - precision: 1.0000 - recall: 0.1050 - auc: 0.8697 - spcsens: 0.6726 - snsspec: 0.8254 - val_loss: 305.2086 - val_tp: 565.0000 - val_fp: 62.0000 - val_tn: 81.0000 - val_fn: 586.0000 - val_weighted_accuracy: 0.5287 - val_precision: 0.9011 - val_recall: 0.4909 - val_auc: 0.5546 - val_spcsens: 0.1678 - val_snsspec: 0.3050\n",
      "decay: 0.89 >> lr: 0.000267\n",
      "Epoch 23/200\n",
      "11644/11644 [==============================] - 8s 695us/sample - loss: 7.7547 - tp: 1352.0000 - fp: 0.0000e+00 - tn: 1283.0000 - fn: 9009.0000 - weighted_accuracy: 0.9042 - precision: 1.0000 - recall: 0.1305 - auc: 0.8764 - spcsens: 0.6812 - snsspec: 0.8320 - val_loss: 264.4461 - val_tp: 561.0000 - val_fp: 62.0000 - val_tn: 81.0000 - val_fn: 590.0000 - val_weighted_accuracy: 0.5270 - val_precision: 0.9005 - val_recall: 0.4874 - val_auc: 0.5440 - val_spcsens: 0.1329 - val_snsspec: 0.2971\n",
      "decay: 0.885 >> lr: 0.0002655\n",
      "Epoch 24/200\n",
      "11644/11644 [==============================] - 8s 689us/sample - loss: 7.0686 - tp: 1573.0000 - fp: 2.0000 - tn: 1281.0000 - fn: 8788.0000 - weighted_accuracy: 0.9052 - precision: 0.9987 - recall: 0.1518 - auc: 0.8720 - spcsens: 0.6952 - snsspec: 0.8382 - val_loss: 223.5820 - val_tp: 552.0000 - val_fp: 61.0000 - val_tn: 82.0000 - val_fn: 599.0000 - val_weighted_accuracy: 0.5266 - val_precision: 0.9005 - val_recall: 0.4796 - val_auc: 0.5603 - val_spcsens: 0.1678 - val_snsspec: 0.3215\n",
      "decay: 0.88 >> lr: 0.00026399999999999997\n",
      "Epoch 25/200\n",
      "11644/11644 [==============================] - 8s 679us/sample - loss: 6.3869 - tp: 1648.0000 - fp: 1.0000 - tn: 1282.0000 - fn: 8713.0000 - weighted_accuracy: 0.9066 - precision: 0.9994 - recall: 0.1591 - auc: 0.8739 - spcsens: 0.6882 - snsspec: 0.8251 - val_loss: 183.5744 - val_tp: 563.0000 - val_fp: 57.0000 - val_tn: 86.0000 - val_fn: 588.0000 - val_weighted_accuracy: 0.5454 - val_precision: 0.9081 - val_recall: 0.4891 - val_auc: 0.5776 - val_spcsens: 0.2098 - val_snsspec: 0.3015\n",
      "decay: 0.875 >> lr: 0.0002625\n",
      "Epoch 26/200\n",
      "11644/11644 [==============================] - 8s 682us/sample - loss: 5.6709 - tp: 1694.0000 - fp: 5.0000 - tn: 1278.0000 - fn: 8667.0000 - weighted_accuracy: 0.9044 - precision: 0.9971 - recall: 0.1635 - auc: 0.8727 - spcsens: 0.6836 - snsspec: 0.8352 - val_loss: 145.5401 - val_tp: 529.0000 - val_fp: 52.0000 - val_tn: 91.0000 - val_fn: 622.0000 - val_weighted_accuracy: 0.5481 - val_precision: 0.9105 - val_recall: 0.4596 - val_auc: 0.5805 - val_spcsens: 0.2308 - val_snsspec: 0.3467\n",
      "decay: 0.87 >> lr: 0.000261\n",
      "Epoch 27/200\n",
      "11644/11644 [==============================] - 8s 699us/sample - loss: 4.9854 - tp: 1614.0000 - fp: 2.0000 - tn: 1281.0000 - fn: 8747.0000 - weighted_accuracy: 0.9056 - precision: 0.9988 - recall: 0.1558 - auc: 0.8736 - spcsens: 0.6968 - snsspec: 0.8450 - val_loss: 110.5074 - val_tp: 505.0000 - val_fp: 54.0000 - val_tn: 89.0000 - val_fn: 646.0000 - val_weighted_accuracy: 0.5307 - val_precision: 0.9034 - val_recall: 0.4387 - val_auc: 0.5768 - val_spcsens: 0.2378 - val_snsspec: 0.3110\n",
      "decay: 0.865 >> lr: 0.00025949999999999997\n",
      "Epoch 28/200\n",
      "11644/11644 [==============================] - 8s 726us/sample - loss: 4.3322 - tp: 1550.0000 - fp: 7.0000 - tn: 1276.0000 - fn: 8811.0000 - weighted_accuracy: 0.9014 - precision: 0.9955 - recall: 0.1496 - auc: 0.8736 - spcsens: 0.6991 - snsspec: 0.8430 - val_loss: 80.7675 - val_tp: 465.0000 - val_fp: 47.0000 - val_tn: 96.0000 - val_fn: 686.0000 - val_weighted_accuracy: 0.5379 - val_precision: 0.9082 - val_recall: 0.4040 - val_auc: 0.5894 - val_spcsens: 0.2797 - val_snsspec: 0.3692\n",
      "decay: 0.86 >> lr: 0.000258\n",
      "Epoch 29/200\n",
      " 5984/11644 [==============>...............] - ETA: 3s - loss: 3.8429 - tp: 733.0000 - fp: 1.0000 - tn: 653.0000 - fn: 4597.0000 - weighted_accuracy: 0.9028 - precision: 0.9986 - recall: 0.1375 - auc: 0.8739 - spcsens: 0.7248 - snsspec: 0.8743"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-aa604b2c4b12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m      \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# The class weights go here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     class_weight=class_weight) \n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weighted_history = weighted_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    sample_weight=train_sample_weights,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "   callbacks = [early_stopping,model_checkpoint,lrscheduler],\n",
    "     validation_data=(X_val, y_val,val_sample_weights),\n",
    "    # The class weights go here\n",
    "    class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "#plot_graphs(weighted_history, \"binacc\")\n",
    "plot_graphs(weighted_history, \"weighted_accuracy\")\n",
    "plot_graphs(weighted_history, \"loss\")\n",
    "plot_graphs(weighted_history, \"precision\")\n",
    "plot_graphs(weighted_history, \"spcsens\")\n",
    "plot_graphs(weighted_history, \"snsspec\")\n",
    "plot_graphs(weighted_history, \"fp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_predictions_weighted = weighted_model.predict(X_train, batch_size=BATCH_SIZE)\n",
    "#test_predictions_weighted = weighted_model.predict(X_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_results = weighted_model.evaluate(X_test, y_test,\n",
    "                                           batch_size=BATCH_SIZE, verbose=0,sample_weight=test_sample_weights)\n",
    "for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "#plot_cm(y_test, test_predictions_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_model=keras.models.load_model(r\"D:\\work\\SavedModels\\Tweedy\\Ckpt\\chkpntwghts.158-0.7733.hdf5\")\n",
    "#chkpntwghts.44-0.7479.hdf5\n",
    "ld_model_results = ld_model.evaluate(X_test, y_test,\n",
    "                                           batch_size=BATCH_SIZE, verbose=0,sample_weight=test_sample_weights)\n",
    "for name, value in zip(weighted_model.metrics_names, ld_model_results ):\n",
    "  print(name, ': ', value)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(954*.56+127*4.54)/((954*.56+127*4.54)+31*4.54+326*.56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ct = datetime.now()\n",
    "ct=('Model_'+str(ct.month).zfill(2)+str(ct.day).zfill(2)+'_'+str(ct.hour).zfill(2)+str(ct.minute).zfill(2))\n",
    "print(ct)\n",
    "weighted_model.save(\"D:\\\\work\\\\SavedModels\\\\Tweedy\\\\Techmodels\\\\\"+ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/tensorflow/tensorflow/blob/f45d6083b766183e045552dafa8e46586eb3d4fc/tensorflow/python/keras/metrics.py#L1500\n",
    "\n",
    "    \n",
    "\n",
    "@keras_export('keras.metrics.SensitivityAtSpecificity')\n",
    "class SensitivityAtSpecificity(SensitivitySpecificityBase):\n",
    "  \"\"\"Computes best sensitivity where specificity is >= specified value.\n",
    "  the sensitivity at a given specificity.\n",
    "  `Sensitivity` measures the proportion of actual positives that are correctly\n",
    "  identified as such (tp / (tp + fn)).\n",
    "  `Specificity` measures the proportion of actual negatives that are correctly\n",
    "  identified as such (tn / (tn + fp)).\n",
    "  This metric creates four local variables, `true_positives`, `true_negatives`,\n",
    "  `false_positives` and `false_negatives` that are used to compute the\n",
    "  sensitivity at the given specificity. The threshold for the given specificity\n",
    "  value is computed and used to evaluate the corresponding sensitivity.\n",
    "  If `sample_weight` is `None`, weights default to 1.\n",
    "  Use `sample_weight` of 0 to mask values.\n",
    "  For additional information about specificity and sensitivity, see\n",
    "  [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
